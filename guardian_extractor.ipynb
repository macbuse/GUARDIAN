{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardian article extractor\n",
    "\n",
    "About 15 years ago I wrote a script to scrape the Guardian newspaper site\n",
    "and write them to a .html so that I could print it off for reading in the\n",
    "toilet at work.\n",
    "\n",
    "This took a couple of weeks to get working correctly. Then they had a\n",
    "couple of modifications and I had to rewrite but it wasn't so hard.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps in the procedure\n",
    "\n",
    "1. Scrape the front page and get the links to stories\n",
    "1. Access the links and scrape each of the pages\n",
    "1. Extract meta data and story text from the pages\n",
    "1. Sort and create table of contents (toc)\n",
    "1. Format and dump to a file\n",
    "\n",
    "### Much easier\n",
    "\n",
    "Things are easier today using **Goose**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started\n",
    "\n",
    "Pull the front page and get the links.\n",
    "Some use BeautifulSoup to do this but regexp\n",
    "is more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, calendar\n",
    "mydate = datetime.datetime.now()\n",
    " \n",
    "yy, mm, dd = str(mydate).split(' ')[0].split('-')\n",
    "today = calendar.month_abbr[int(mm)].lower() + '/' + dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "pp = re.compile('<a href=\"(.*?)\".*?data-link-name=\"(.*?)\"', re.DOTALL)\n",
    "r = requests.get('https://www.theguardian.com/')\n",
    "\n",
    "\n",
    "lks =  set([ lk for lk, name in pp.findall(r.text) \n",
    "                       if name == 'article' and today in lk ])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.theguardian.com/world/2019/aug/07/japanese-food-store-closes-after-videos-of-rats-browsing-its-shelves-go-viral',\n",
       " 'https://www.theguardian.com/environment/2019/aug/07/stare-seagulls-out-to-save-your-snacks-researcher-says',\n",
       " 'https://www.theguardian.com/commentisfree/2019/aug/07/fossil-fuel-lobby-pollute-politics-climate-crisis']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lks)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting the links\n",
    "\n",
    "I always write a class to do this.\n",
    "The object contains :\n",
    "\n",
    "- meta data\n",
    "- a payload\n",
    "- a unique identifier generated using a hash fucntion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "class GuardianArt():\n",
    "    \n",
    "    def __init__(self, lk=\"\"):\n",
    "        self.raw = lk\n",
    "        self.id = str(hash(self))\n",
    "        \n",
    "        #this is the payload\n",
    "        self.goose_art = None\n",
    "        \n",
    "        #the rest is meta data\n",
    "        self.label = lk.split('/')[3]\n",
    "        self.tag = 'misc'\n",
    "        \n",
    "        \n",
    "        #change tag from misc if appropriate to do so \n",
    "        if ( lk.split('/')[4] in ['audio', 'video', 'gallery']):\n",
    "            self.tag = self.label = 'xmedia'    \n",
    "        elif (self.label == 'world' or 'news' in self.label):\n",
    "            self.tag = 'news'\n",
    "        elif self.label == 'commentisfree':\n",
    "            self.tag = 'op_ed'\n",
    "\n",
    "    def __hash__(self):\n",
    "        return int( hashlib.md5(self.raw.encode()).hexdigest(), 16)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.raw\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.raw\n",
    "\n",
    "\n",
    "arts = [GuardianArt(lk) for lk in lks]\n",
    "\n",
    "shit_list = ['xmedia', 'football', 'business', 'stage', 'sport'] \n",
    "arts = [art for art in arts \n",
    "             if art.label not in shit_list]\n",
    "\n",
    "#tags = set([art.tag for art in arts])\n",
    "misc = set([art.label for art in arts if art.tag == 'misc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the articles\n",
    "\n",
    "This used to be hard but I use Goose to do this now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "japanese-food-store-closes-after-videos-of-rats-browsing-its-shelves-go-viral\n",
      "stare-seagulls-out-to-save-your-snacks-researcher-says\n",
      "fossil-fuel-lobby-pollute-politics-climate-crisis\n",
      "no-deal-brexit-would-harm-uk-security-senior-officer-warns\n",
      "i-am-really-shy-introducing-phoenix-the-worlds-first-hijab-wearing-champion-wrestler\n",
      "high-speed-trains-to-nowhere-australias-long-running-rail-fail\n",
      "post-referendum-britain-royals-progressive-meghan-markle\n",
      "high-court-rules-public-servants-can-be-sacked-for-political-social-media-posts\n",
      "fight-for-our-lives-fiji-calls-world-leaders-selfish-as-it-lays-out-climate-crisis-blueprint\n",
      "hong-kong-protests-australia-issues-travel-alert-as-china-warns-of-worst-crisis-since-1997\n",
      "salisbury-attack-metropolitan-police-examine-role-vladimir-putin-russia\n",
      "kathy-burke-after-i-got-sick-the-toughest-thing-was-what-it-did-to-my-mental-health\n",
      "bondi-beach-mural-of-border-force-officers-defaced-after-council-vote-to-keep-it\n",
      "canada-manhunt-police-find-several-items-linked-to-suspects\n",
      "fossils-largest-parrot-ever-recorded-found-new-zealand-heracles-inexpectatus\n",
      "philippines-declares-epidemic-after-dengue-fever-kills-more-than-600\n",
      "papua-new-guinea-asks-china-to-refinance-its-national-debt-as-beijing-influence-grows\n",
      "camra-calls-time-on-sexist-names-at-great-british-beer-festival\n",
      "spain-road-trip-granada-alhambra-almeria-spaghetti-western\n",
      "budget-airline-disasters-low-cost-flying-nightmares\n",
      "masked-men-destroy-hong-kong-lennon-wall-at-australias-university-of-queensland\n"
     ]
    }
   ],
   "source": [
    "from goose3 import Goose\n",
    "#from  bs4 import BeautifulSoup\n",
    "\n",
    "gg = Goose()\n",
    "\n",
    "for art in arts:\n",
    "    print(  str(art).split('/')[-1],)\n",
    "    try:\n",
    "        art.goose_art = gg.extract(url=str(art) ) \n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to HTML\n",
    "\n",
    "There are libraries for doing HTML output from python \n",
    "but as often as not string formatting is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = '''<!DOCTYPE html>\n",
    "<html id=\"js-context\" class=\"js-off is-not-modern id--signed-out\" lang=\"en\" data-page-path=\"/international\">\n",
    "<head>\n",
    "<title>News, sport and opinion from the Guardian's global edition | The Guardian</title>\n",
    "<meta charset=\"utf-8\">\n",
    "</head>\n",
    "'''  \n",
    "\n",
    "headline_wrapper = '<h2 id=\"%s\">%s</h2>'\n",
    "keyword_wrapper = '<h4>%s</h4>'\n",
    "para_wrapper = \"<p>%s</p>\\n\\n\"\n",
    "toc_item_wrapper = '<a href=\"#%s\">%s</a><br>\\n'\n",
    "\n",
    "sections = ['news', 'op_ed', 'misc']\n",
    "text  = {x:[] for x in sections}\n",
    "tocs  = {x:[] for x in sections}\n",
    "\n",
    "\n",
    "for yy in arts:\n",
    "    #the goose object is stored in the article\n",
    "    art = yy.goose_art\n",
    "    \n",
    "    html_id = (yy.id, art.title)\n",
    "    tocs[yy.tag].append(html_id) \n",
    "    \n",
    "    html_text = ''.join( [para_wrapper%x for x in art.cleaned_text.split('\\n'*2)])\n",
    "    text[yy.tag].append('\\n\\n'.join([ headline_wrapper%html_id, \n",
    "                                      keyword_wrapper%art.meta_keywords, \n",
    "                                      html_text]))\n",
    "\n",
    "#write to a file remember to use a context cos it's cleaner   \n",
    "with  open('text.html','w') as fp:\n",
    "    fp.write(header + '<body>\\n')\n",
    "    \n",
    "    for ss in sections:\n",
    "        fp.write('<h1>%s</h1>'%ss.upper())\n",
    "        toc_html = [toc_item_wrapper%x for x in tocs[ss] ]\n",
    "        fp.write(''.join(toc_html) + '<br>\\n')\n",
    "        \n",
    "    for ss in sections:\n",
    "        fp.write('\\n'.join(text[ss]) )\n",
    "\n",
    "    fp.write('</body>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
